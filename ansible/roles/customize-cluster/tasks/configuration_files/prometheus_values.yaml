server:
  persistentVolume:
    size: 25Gi

serverFiles:
  alerting_rules.yml:
    groups:
    - name: k8s alerts
      rules:
      - alert: Kubernetes node down
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Node {{ $labels.node }} is down
          Description: Failed to scrape {{ $labels.node }} for more than 3 minutes. Node seems down.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: Target job is down
        expr: (up {kubernetes_name!="prometheus-node-exporter"}) == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Target job={{ $labels.job }}, app={{ $labels.app }}, namespace={{ $labels.kubernetes_namespace }} on instance {{ $labels.instance }} is down
          Description: Failed to scrape {{ $labels.node }} for more than 3 minutes. Node seems down.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Disk Pressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Kubernetes disk pressure (instance {{ $labels.node }})
          Description: "{{ $labels.instance }} has DiskPressure condition."
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Disk usage in node is at 85%
        expr: (100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} * 100) / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"})) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Disk usage on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Disk usage in node is at 85%
        expr: (100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} * 100) / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"})) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Disk usage on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Disk usage in node is at 95%
        expr: (100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} * 100) / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"})) > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          Summary: Disk usage on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Volume Full In Four Days
        expr: predict_linear(kubelet_volume_stats_available_bytes{namespace!~"pre-.*|des-.*|pro-.*"}[4d], 4 * 24 * 3600) < 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Volume {{ $labels.persistentvolumeclaim }} full in four days
          Description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}"
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Normal"
      - alert: Persistent Volume Error
        expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending"} > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Low space in PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }}
          Description: The PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using more than 85% of its space
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Persistent volume has low free space (> 85% used)
        expr: ((kubelet_volume_stats_used_bytes{namespace!~"pre-.*|des-.*|pro-.*"} / kubelet_volume_stats_capacity_bytes) * 100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Low space in PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }}
          Description: The PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using more than 85% of its space
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: CPU utilization in node higher than 85%
        expr: (100 - (avg by (node) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: CPU utilization is high on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: CPU utilization in pod is at 85%
        expr: (sum(rate(container_cpu_usage_seconds_total{container_name!="POD", namespace!~"pre-.*|des-.*|pro-.*", pod!~".*backrest.*|.*node-agent.*"}[5m])) by (pod, namespace) / ((sum by(pod,namespace) (kube_pod_container_resource_limits_cpu_cores))) * 100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: CPU utilization is high on pod {{ $labels.pod }}
          Description: Pod {{ $labels.pod }} CPU utilization is now at {{ $value }}%
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Normal"
      - alert: Kubernetes node memory utilization is at 90%
        expr: ((1 - (node_memory_MemAvailable_bytes{job!~".*node_exporter.*"} / (node_memory_MemTotal_bytes)))*100) > 90
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Memory utilization is high on {{ $labels.node }}
          Description: Instance {{ $labels.instance }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Node memory utilization is at 90%
        expr: ((1 - (node_memory_MemAvailable_bytes{job=~".*node_exporter.*"} / (node_memory_MemTotal_bytes)))*100) > 90
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Memory utilization is high on {{ $labels.job }}
          Description: Node {{ $labels.instance }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Nodes availables
        expr: sum(up{component="node-exporter"}) < 7
        for: 1m
        labels:
          severity: critical
        annotations:
          Summary: One or more nodes are down
          Description: The current number of current nodes running is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: Low statefulset replicas running
        expr: (kube_statefulset_status_replicas_ready{namespace!~"pre-.*|des-.*|pro-.*"} / kube_statefulset_status_replicas_current) < 1
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: StatefulSet down (instance {{ $labels.node }})
          Description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  StatefulSet = {{ $labels.statefulset }}"
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Low deployment replicas running
        expr: kube_deployment_status_replicas{namespace!~"pre-.*|des-.*|pro-.*"} > kube_deployment_status_replicas_available
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: A pod is running low on replicas
          Description: Some of the pod deployments failed on {{ $labels.deployment }}. The current number of replicas running is {{ $value }}
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Restart loop in a pod
        expr: rate(kube_pod_container_status_restarts_total{namespace!~"pre-.*|des-.*|pro-.*"}[5m]) > 3
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Pod {{ $labels.pod }} is in a restart loop
          Description: The pod {{ $labels.pod }} is in a restart loop in the last 5 minutes
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Pod terminated with errors
        expr: kube_pod_container_status_terminated_reason{reason!~"Completed", namespace!~"pre-.*|des-.*|pro-.*", pod!~".*backrest.*"} == 1
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: The pod {{  $labels.pod  }} terminated unexpected
          Description: The pod {{  $labels.pod  }} terminated by reason {{  $labels.reason  }}
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Not all ETCD pods are running
        expr: (sum(kube_pod_container_info{container="etcd"})) < 3
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: ETCD container status
          Description: The current number of ETCD containers is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: API server status
        expr: kube_pod_container_status_running{container=~"kube-apiserver"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: API server status
          Description: The API server stopped running
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Pod memory utilization is at 85%
        expr: ((avg (container_memory_working_set_bytes{namespace!~"pre-.*|des-.*|pro-.*"}) by (pod))/ on (pod)(avg (container_spec_memory_limit_bytes>0 ) by (pod))*100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Memory utilization in pod {{ $labels.pod }}
          Description: The pod {{ $labels.pod }} memory utilization is currently at {{ $value }} of its memory capacity
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Service is down
        expr: kube_service_info{namespace!~"pre-.*|des-.*|pro-.*"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Service {{  $labels.service  }} seems to be down
          Description: The service {{  $labels.service  }} doesn't respond
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Service: "{{ $labels.service }}"
          Project: "tfm-monitoring"
          Priority: "High"
    - name: PostgresSQL alerts
      rules:
      - alert: DB conflicts higher than 20
        expr: sum(pg_stat_database_conflicts) by (server) > 20
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: The number of database conflicts is higher than 20
          Description: The current number of conflicts is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: DB Locks higher than 50
        expr: sum(pg_locks_count) by (server) > 50
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: The number of locks is higher than 50
          Description: The current number of locks is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: Postgres is down
        expr: up{job="postgres_exporter"} == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Postgres on instance {{ $labels.instance }} is down
          Description: Failed to scrape {{ $labels.instance }} for more than 3 minutes. Postgres seems down.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Current connections in DB is at 85%
        expr: ((sum(pg_settings_superuser_reserved_connections) by (server) + sum(pg_stat_activity_count) by (server)) / sum(pg_settings_max_connections) by (server)) * 100 > 85
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Postgres connections in instance {{ $labels.instance }} is now at {{ $value }} of capacity
          Description: If instance is postgres-exporter alarm is from des and if instance is pre-postgres-exporter is from pre
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: Current connections in DB 100%
        expr: ((sum(pg_settings_superuser_reserved_connections) by (server) + sum(pg_stat_activity_count) by (server)) / sum(pg_settings_max_connections) by (server)) * 100 > 99
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Postgres connections in instance {{ $labels.instance }} is now at {{ $value }} of capacity
          Description: Number of connections more than 100%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: Replication lag
        expr: pg_replication_lag > 10
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Replication lag in DB {{ $labels.server }} is greater than 10 seconds in the last 3 minutes.
          Description: A high replication lag rate can lead to coherence problems if the master goes down
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"