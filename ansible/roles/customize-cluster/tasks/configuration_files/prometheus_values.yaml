##### Grafana #####
grafana:
  # Provision grafana-dashboards-kubernetes
  defaultDashboardsEnabled: false
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'grafana-dashboards-kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-system-api-server:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
        token: ''
      k8s-system-coredns:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
        token: ''
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ''
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ''
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ''
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ''

##### Alertmanager #####
alertmanager:
  enabled: true
  config:
    route:
      group_by: ['alertname']
      group_wait: 0s
      group_interval: 5m
      repeat_interval: 12h
      routes:
      - receiver: 'autoremediate'
      receiver: "autoremediate"
    receivers:
    - name: 'autoremediate'
      webhook_configs:
      - send_resolved: false
        url: http://k8s-api.infra.svc.cluster.local/api/

##### Prometheus #####
defaultRules:
  create: false

prometheusOperator:
  tls:
    enabled: false
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false

prometheus:
  prometheusSpec:
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 25Gi

additionalPrometheusRulesMap:
  prometheus.rules:
    groups:
    - name: k8s-alerts
      rules:
      - alert: KubernetesNodeDown
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Node {{ $labels.node }} is down
          Description: Failed to scrape {{ $labels.node }} for more than 3 minutes. Node seems down.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: DiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Kubernetes disk pressure (instance {{ $labels.node }})
          Description: "{{ $labels.instance }} has DiskPressure condition."
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: DiskUsageInNodeAt85
        expr: (100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} * 100) / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"})) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Disk usage on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: DiskUsageInNodeAt95
        expr: (100 - ((node_filesystem_avail_bytes{mountpoint="/",fstype!="rootfs"} * 100) / node_filesystem_size_bytes{mountpoint="/",fstype!="rootfs"})) > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          Summary: Disk usage on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: VolumeFullInFourDays
        expr: predict_linear(kubelet_volume_stats_available_bytes{namespace!~"pre-.*|des-.*|pro-.*"}[4d], 4 * 24 * 3600) < 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Volume {{ $labels.persistentvolumeclaim }} full in four days
          Description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}"
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Normal"
      - alert: PersistentVolumeError
        expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending"} > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Low space in PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }}
          Description: The PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using more than 85% of its space
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: PersistentVolumeHasLowFreeSpace
        expr: ((kubelet_volume_stats_used_bytes{namespace!~"pre-.*|des-.*|pro-.*"} / kubelet_volume_stats_capacity_bytes) * 100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Low space in PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }}
          Description: The PV {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using more than 85% of its space
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: CPUUtilizationInNodeHigherThan85
        expr: (100 - (avg by (node) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: CPU utilization is high on {{ $labels.node }}
          Description: Node {{ $labels.node }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: CPUUtilizationInPodIsAt85
        expr: (sum(rate(container_cpu_usage_seconds_total{container_name!="POD", namespace!~"pre-.*|des-.*|pro-.*", pod!~".*backrest.*|.*node-agent.*"}[5m])) by (pod, namespace) / ((sum by(pod,namespace) (kube_pod_container_resource_limits_cpu_cores))) * 100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: CPU utilization is high on pod {{ $labels.pod }}
          Description: Pod {{ $labels.pod }} CPU utilization is now at {{ $value }}%
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Normal"
      - alert: KubernetesNodeMemoryUtilizationIsAt90
        expr: ((1 - (node_memory_MemAvailable_bytes{job!~".*node_exporter.*"} / (node_memory_MemTotal_bytes)))*100) > 90
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Memory utilization is high on {{ $labels.node }}
          Description: Instance {{ $labels.instance }} is now at {{ $value }}%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: NodesAvailables
        expr: sum(up{component="node-exporter"}) < 5
        for: 1m
        labels:
          severity: critical
        annotations:
          Summary: One or more nodes are down
          Description: The current number of current nodes running is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: LowStatefulsetReplicasRunning
        expr: (kube_statefulset_status_replicas_ready{namespace!~"pre-.*|des-.*|pro-.*"} / kube_statefulset_status_replicas_current) < 1
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: StatefulSet down (instance {{ $labels.node }})
          Description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  StatefulSet = {{ $labels.statefulset }}"
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: LowDeploymentReplicasRunning
        expr: kube_deployment_status_replicas{namespace!~"pre-.*|des-.*|pro-.*"} > kube_deployment_status_replicas_available
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: A pod is running low on replicas
          Description: Some of the pod deployments failed on {{ $labels.deployment }}. The current number of replicas running is {{ $value }}
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: RestartLoopInPod
        expr: rate(kube_pod_container_status_restarts_total{namespace!~"pre-.*|des-.*|pro-.*"}[5m]) > 3
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Pod {{ $labels.pod }} is in a restart loop
          Description: The pod {{ $labels.pod }} is in a restart loop in the last 5 minutes
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: PodTerminatedWithErrors
        expr: kube_pod_container_status_terminated_reason{reason!~"Completed", namespace!~"pre-.*|des-.*|pro-.*", pod!~".*backrest.*"} == 1
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: The pod {{  $labels.pod  }} terminated unexpected
          Description: The pod {{  $labels.pod  }} terminated by reason {{  $labels.reason  }}
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: NotAllETCDPodsAreRunning
        expr: (sum(kube_pod_container_info{container="etcd"})) < 3
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: ETCD container status
          Description: The current number of ETCD containers is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: APIServerStatus
        expr: kube_pod_container_status_running{container=~"kube-apiserver"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: API server status
          Description: The API server stopped running
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: PodMemoryUtilizationIsAt85
        expr: ((avg (container_memory_working_set_bytes{namespace!~"pre-.*|des-.*|pro-.*"}) by (pod))/ on (pod)(avg (container_spec_memory_limit_bytes>0 ) by (pod))*100) > 85
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: Memory utilization in pod {{ $labels.pod }}
          Description: The pod {{ $labels.pod }} memory utilization is currently at {{ $value }} of its memory capacity
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Pod: "{{ $labels.pod }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: ServiceIsDown
        expr: kube_service_info{namespace!~"pre-.*|des-.*|pro-.*"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Service {{  $labels.service  }} seems to be down
          Description: The service {{  $labels.service  }} doesn't respond
          Environment: "DES"
          Namespace: "{{ $labels.namespace }}"
          Service: "{{ $labels.service }}"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: PodUnschedulable
        expr: kube_pod_status_phase{phase="Pending"} == 1
        for: 1s
        labels:
          severity: critical
        annotations:
          Summary: Pod {{ $labels.pod }} is unschedulable
          Description: Pod {{ $labels.pod }} has been unschedulable for more than 3 minutes.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
    - name: PostgresSQL-alerts
      rules:
      - alert: DBConflictsHigherThan20
        expr: sum(pg_stat_database_conflicts) by (server) > 20
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: The number of database conflicts is higher than 20
          Description: The current number of conflicts is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: DBLocksHigherThan50
        expr: sum(pg_locks_count) by (server) > 50
        for: 1m
        labels:
          severity: warning
        annotations:
          Summary: The number of locks is higher than 50
          Description: The current number of locks is {{ $value }}
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "High"
      - alert: PostgresIsDown
        expr: up{job="postgres_exporter"} == 0
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Postgres on instance {{ $labels.instance }} is down
          Description: Failed to scrape {{ $labels.instance }} for more than 3 minutes. Postgres seems down.
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: CurrentConnectionsInDBIsAt85
        expr: ((sum(pg_settings_superuser_reserved_connections) by (server) + sum(pg_stat_activity_count) by (server)) / sum(pg_settings_max_connections) by (server)) * 100 > 85
        for: 3m
        labels:
          severity: warning
        annotations:
          Summary: Postgres connections in instance {{ $labels.instance }} is now at {{ $value }} of capacity
          Description: If instance is postgres-exporter alarm is from des and if instance is pre-postgres-exporter is from pre
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Urgent"
      - alert: CurrentConnectionsInDB100
        expr: ((sum(pg_settings_superuser_reserved_connections) by (server) + sum(pg_stat_activity_count) by (server)) / sum(pg_settings_max_connections) by (server)) * 100 > 99
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Postgres connections in instance {{ $labels.instance }} is now at {{ $value }} of capacity
          Description: Number of connections more than 100%
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
      - alert: ReplicationLag
        expr: pg_replication_lag > 10
        for: 3m
        labels:
          severity: critical
        annotations:
          Summary: Replication lag in DB {{ $labels.server }} is greater than 10 seconds in the last 3 minutes.
          Description: A high replication lag rate can lead to coherence problems if the master goes down
          Environment: "DES"
          Project: "tfm-monitoring"
          Priority: "Immediate"
